{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mbaroudi/DELTA_LAKE_TIPS/blob/main/Delta_Lake_Tips_SCD7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction to Slowly Changing Dimension (SCD) Type 7\n",
        "In the context of PySpark and using Delta Lake to manage dimensions with slowly changing dimension strategies, the concept of a **\"Type 7 Dimension\"** is not standard. Typically, types 1 through 6 cover various scenarios of handling dimensional data changes. However, \"Type 7\" often refers to a **hybrid approach** combining the features of Type 1 (overwriting old data) and Type 2 (tracking historical data with version management). This hybrid approach allows querying both the current state of the data and its historical versions.\n",
        "\n",
        "In this notebook, we will implement a Type 7 SCD using PySpark and Delta Lake by essentially employing a **Type 2 SCD** but also maintaining a current view for easier access to the latest records."
      ],
      "metadata": {
        "id": "Rn81tCsl_jtR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5TUcEV3q_e9b",
        "outputId": "f39ca249-fa5b-4999-9583-ac840f0b5d5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=78645a1f225b8e9e780e160038e97f693a622b52ee9f9e53cb3e97f9131a71a4\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"SCD Type 7 with Temporal Aggregates\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparing the Data\n",
        "First, initialize your DataFrame with a set of customer data. This data includes several fields: customer ID, name, email, revenue, effective date, and an indicator of whether the record is current. This setup prepares us for implementing SCD operations."
      ],
      "metadata": {
        "id": "ue55LbypBDgS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession, functions as F\n",
        "\n",
        "# Initialize Spark Session again (redundant, just for clarity in documentation)\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"SCD Type 7 with Temporal Aggregates\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Sample data with 'is_current' column added\n",
        "data = [\n",
        "    (1, \"John Doe\", \"john.doe@email.com\", 1000, \"2020-01-15\", True),\n",
        "    (2, \"Jane Smith\", \"jane.smith@email.com\", 1500, \"2020-01-20\", True)\n",
        "]\n",
        "columns = [\"customer_id\", \"customer_name\", \"email\", \"revenue\", \"effective_date\", \"is_current\"]\n",
        "df = spark.createDataFrame(data, schema=columns)\n",
        "\n",
        "df.show()\n"
      ],
      "metadata": {
        "id": "yh61dpQTBN0i",
        "outputId": "bf3cdb71-35d6-42aa-e5d2-c54b09c7b6ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+--------------------+-------+--------------+----------+\n",
            "|customer_id|customer_name|               email|revenue|effective_date|is_current|\n",
            "+-----------+-------------+--------------------+-------+--------------+----------+\n",
            "|          1|     John Doe|  john.doe@email.com|   1000|    2020-01-15|      true|\n",
            "|          2|   Jane Smith|jane.smith@email.com|   1500|    2020-01-20|      true|\n",
            "+-----------+-------------+--------------------+-------+--------------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Applying SCD Type 7\n",
        "Use the following code to simulate the operations of SCD Type 7. This involves creating a function to handle the merging of historical and new data, maintaining a view for the latest records."
      ],
      "metadata": {
        "id": "_ZO13bXRBTzr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_scd_type_2(base_df, updates_df):\n",
        "    # Join based on customer_id and check for changes\n",
        "    condition = (base_df[\"customer_id\"] == updates_df[\"customer_id\"]) & \\\n",
        "                (base_df[\"is_current\"] == True) & \\\n",
        "                ((base_df[\"customer_name\"] != updates_df[\"customer_name\"]) |\n",
        "                 (base_df[\"revenue\"] != updates_df[\"revenue\"]))\n",
        "\n",
        "    # Set existing records to not current if changes are detected\n",
        "    updates_df = updates_df.withColumn(\"is_current\", F.lit(True))\n",
        "    updated_existing_df = base_df.join(updates_df, \"customer_id\", \"inner\") \\\n",
        "                                 .filter(condition) \\\n",
        "                                 .select(base_df[\"*\"]) \\\n",
        "                                 .withColumn(\"is_current\", F.lit(False))\n",
        "\n",
        "    # Union all: unchanged existing, updated existing set to false, and new updates set to true\n",
        "    final_df = base_df.join(updated_existing_df, [\"customer_id\"], \"left_anti\") \\\n",
        "                      .unionByName(updated_existing_df) \\\n",
        "                      .unionByName(updates_df)\n",
        "\n",
        "    return final_df\n",
        "\n",
        "historical_df = apply_scd_type_2(df, new_df)\n",
        "historical_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-z0yoNEnBcO3",
        "outputId": "c4d2b12f-6b59-4c47-eac7-026d42e9495a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+--------------------+-------+--------------+----------+\n",
            "|customer_id|customer_name|               email|revenue|effective_date|is_current|\n",
            "+-----------+-------------+--------------------+-------+--------------+----------+\n",
            "|          2|   Jane Smith|jane.smith@email.com|   1500|    2020-01-20|      true|\n",
            "|          1|     John Doe|  john.doe@email.com|   1000|    2020-01-15|     false|\n",
            "|          1|Johnathan Doe|  john.doe@email.com|   1200|    2020-02-01|      true|\n",
            "|          2|   Jane Smith|jane.smith@email.com|   1500|    2020-02-01|      true|\n",
            "|          3|   Mike Jones|mike.jones@email.com|    500|    2020-02-01|      true|\n",
            "+-----------+-------------+--------------------+-------+--------------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analyzing Temporal Aggregates\n",
        "Next, compute the temporal aggregates such as MTD (Month-to-Date), QTD (Quarter-to-Date), and YTD (Year-to-Date) revenue. This demonstrates how to extract actionable insights from historical data."
      ],
      "metadata": {
        "id": "3bKJ1yhPDQWA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating temporal aggregates\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "historical_df = historical_df.withColumn(\"month\", F.month(\"effective_date\"))\n",
        "historical_df = historical_df.withColumn(\"quarter\", F.quarter(\"effective_date\"))\n",
        "historical_df = historical_df.withColumn(\"year\", F.year(\"effective_date\"))\n",
        "\n",
        "historical_df.createOrReplaceTempView(\"historical_data\")\n",
        "\n",
        "# SQL query for MTD, QTD, YTD calculations\n",
        "aggregates_query = \"\"\"\n",
        "SELECT customer_id, customer_name,\n",
        "       SUM(CASE WHEN month = 2 AND year = 2020 THEN revenue ELSE 0 END) as MTD_Revenue,\n",
        "       SUM(CASE WHEN quarter = (SELECT quarter FROM historical_data WHERE month = 2 AND year = 2020 LIMIT 1) AND year = 2020 THEN revenue ELSE 0 END) as QTD_Revenue,\n",
        "       SUM(CASE WHEN year = 2020 THEN revenue ELSE 0 END) as YTD_Revenue\n",
        "FROM historical_data\n",
        "GROUP BY customer_id, customer_name\n",
        "\"\"\"\n",
        "aggregates_df = spark.sql(aggregates_query)\n",
        "aggregates_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4Wo7JtIDPI1",
        "outputId": "ef41295d-eb8b-48a7-f6b1-8750a4ecde75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+-----------+-----------+-----------+\n",
            "|customer_id|customer_name|MTD_Revenue|QTD_Revenue|YTD_Revenue|\n",
            "+-----------+-------------+-----------+-----------+-----------+\n",
            "|          2|   Jane Smith|       1500|       3000|       3000|\n",
            "|          1|     John Doe|          0|       1000|       1000|\n",
            "|          1|Johnathan Doe|       1200|       1200|       1200|\n",
            "|          3|   Mike Jones|        500|        500|        500|\n",
            "+-----------+-------------+-----------+-----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Writing Historical Data\n",
        "Finally, demonstrate writing the historical data, including all versions, to a Hive table. This ensures that all data changes are preserved and can be audited or analyzed later."
      ],
      "metadata": {
        "id": "llTP_4hzG1w6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Storing data in a Hive fact table\n",
        "aggregates_df.write.mode(\"overwrite\").saveAsTable(\"fact_revenue_aggregates\")\n",
        "# To append data instead of overwriting, you could use mode(\"append\")"
      ],
      "metadata": {
        "id": "Pj96lDeBGgJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This documentation guides you through the setup and application of a Type 7 SCD using PySpark and Delta Lake in a Colab environment. It is structured for easy replication and adaptation for similar analytics needs."
      ],
      "metadata": {
        "id": "summary"
      }
    }
  ]
}