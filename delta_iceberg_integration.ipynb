{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mbaroudi/DELTA_LAKE_TIPS/blob/main/delta_iceberg_integration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Integrating Delta Lake and Apache Iceberg with Hive\n",
        "\n",
        "In this notebook, we explore how to use Delta Lake and Apache Iceberg in conjunction with Hive to create a robust data lakehouse architecture. This setup allows for the use of the same shared storage layer for Hive external tables, Delta Lake, and Iceberg tables, thus minimizing the impact on the existing big data environment.\n",
        "\n",
        "We will demonstrate how to configure Spark to work with both Delta Lake and Apache Iceberg and how to perform data operations that are reflective of real-world data workflows."
      ],
      "metadata": {
        "id": "intro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initial Setup\n",
        "\n",
        "First, we need to install and configure the necessary libraries and sessions for using Spark, Delta Lake, and Iceberg."
      ],
      "metadata": {
        "id": "setup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark==3.1.2 delta-spark iceberg-spark\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Delta Lake and Iceberg Integration\") \\\n",
        "    .config(\"spark.jars.packages\", 'io.delta:delta-core_2.12:1.0.0,org.apache.iceberg:iceberg-spark3-runtime:0.13.1') \\\n",
        "    .config(\"spark.sql.extensions\", 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,io.delta.sql.DeltaSparkSessionExtension') \\\n",
        "    .config(\"spark.sql.catalog.spark_catalog\", 'org.apache.spark.sql.delta.catalog.DeltaCatalog,org.apache.iceberg.spark.SparkSessionCatalog') \\\n",
        "    .config(\"spark.sql.catalog.spark_catalog.type\", \"hadoop\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n"
      ],
      "metadata": {
        "id": "install"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating a Unified Storage Layer\n",
        "\n",
        "Delta Lake and Apache Iceberg can share the same underlying storage, which can be a HDFS directory, an AWS S3 bucket, or any other storage system compatible with Hadoop. This shared access ensures that data managed by Delta Lake or Iceberg can also be accessed as external Hive tables without duplicating data or creating data silos."
      ],
      "metadata": {
        "id": "storage-layer"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example: Writing and Reading Data\n",
        "\n",
        "We will create tables using both Delta Lake and Apache Iceberg, perform write and read operations, and show how these tables can be accessed via Hive."
      ],
      "metadata": {
        "id": "example"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a path for Delta and Iceberg tables\n",
        "path = '/tmp/delta_iceberg_tables'\n",
        "\n",
        "# Creating a Delta Table\n",
        "df = spark.range(0, 5)\n",
        "df.write.format('delta').save(path + '/delta_table')\n",
        "\n",
        "# Creating an Iceberg Table\n",
        "df.write.format('iceberg').save(path + '/iceberg_table')\n"
      ],
      "metadata": {
        "id": "create-tables"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Accessing Data via Hive\n",
        "\n",
        "With the tables stored in a common directory, we can easily set up external Hive tables pointing to this location. This allows tools that query Hive to access up-to-date data without direct interaction with Delta Lake or Iceberg."
      ],
      "metadata": {
        "id": "hive-access"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusion\n",
        "\n",
        "This integration highlights the flexibility and efficiency of using Delta Lake and Apache Iceberg in a unified architecture. By leveraging a common storage layer and compatible configurations, we can simplify the management of large-scale data environments, enhance performance, and ensure consistency across different data management platforms."
      ],
      "metadata": {
        "id": "conclusion"
      }
    }
  ]
}