{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mbaroudi/DELTA_LAKE_TIPS/blob/main/apache_iceberg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# Advanced BI Techniques with Apache Iceberg and Spark SQL\n",
        "\n",
        "This notebook demonstrates advanced capabilities of Apache Iceberg in a BI context, including handling schema evolution, versioning, and complex time-based aggregations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup"
      },
      "outputs": [],
      "source": [
        "!pip install pyspark==3.1.2 iceberg-spark\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize Spark session with Iceberg support\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Advanced BI with Iceberg\") \\\n",
        "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
        "    .config(\"spark.sql.catalog.local\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
        "    .config(\"spark.sql.catalog.local.type\", \"hadoop\") \\\n",
        "    .config(\"spark.sql.catalog.local.warehouse\", \"file:///tmp/iceberg-warehouse\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Create an Iceberg table for customers with schema evolution and history tracking enabled\n",
        "spark.sql(\"\"\"\n",
        "CREATE TABLE IF NOT EXISTS local.db.customers (\n",
        "    id int,\n",
        "    name string,\n",
        "    email string,\n",
        "    revenue double,\n",
        "    updated timestamp,\n",
        "    is_current boolean\n",
        ") USING iceberg PARTITIONED BY (days(updated))\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "data-loading"
      },
      "source": [
        "## Data Loading and Versioning\n",
        "\n",
        "Load data into the Iceberg table and simulate updates to demonstrate versioning and schema evolution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load-data",
        "outputId": "a1bcac85-8c13-464c-af8a-62011f66b577"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial data loaded.\n",
            "Updates applied with overwrite.\n"
          ]
        }
      ],
      "source": [
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Load initial data\n",
        "initial_data = spark.createDataFrame([\n",
        "    (1, 'Alice', 'alice@example.com', 100.0, datetime.now(), True),\n",
        "    (2, 'Bob', 'bob@example.com', 150.0, datetime.now() - timedelta(days=1), True)\n",
        "], ['id', 'name', 'email', 'revenue', 'updated', 'is_current'])\n",
        "\n",
        "initial_data.write.format('iceberg').mode('append').save('local.db.customers')\n",
        "print('Initial data loaded.')\n",
        "\n",
        "# Simulating updates\n",
        "updates = spark.createDataFrame([\n",
        "    (1, 'Alice', 'alice_new@example.com', 200.0, datetime.now(), True),\n",
        "    (2, 'Bob', 'bob_new@example.com', 300.0, datetime.now(), True)\n",
        "], ['id', 'name', 'email', 'revenue', 'updated', 'is_current'])\n",
        "\n",
        "updates.write.format('iceberg').mode('overwrite').option('overwrite-mode', 'dynamic').save('local.db.customers')\n",
        "print('Updates applied with overwrite.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "periodical-calculations"
      },
      "source": [
        "## Periodical Calculations (MTD, QTD, YTD)\n",
        "\n",
        "Calculate monthly, quarterly, and yearly totals using the current data snapshot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "calculate-periodicals",
        "outputId": "c9d47c8b-a9a8-4f96-aeae-335748ebd662"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+-----------+-----------+\n",
            "|MTD_Revenue|QTD_Revenue|YTD_Revenue|\n",
            "+-----------+-----------+-----------+\n",
            "|      300.0|      300.0|      600.0|\n",
            "+-----------+-----------+-----------+\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions as F\n",
        "current_date = datetime.now()\n",
        "start_of_month = current_date.replace(day=1)\n",
        "start_of_quarter = current_date.replace(month=(current_date.month-1)//3*3+1, day=1)\n",
        "start_of_year = current_date.replace(month=1, day=1)\n",
        "\n",
        "revenue_stats = spark.sql(\"\"\"\n",
        "SELECT\n",
        "    SUM(CASE WHEN updated >= '{0}' THEN revenue ELSE 0 END) as MTD_Revenue,\n",
        "    SUM(CASE WHEN updated >= '{1}' THEN revenue ELSE 0 END) as QTD_Revenue,\n",
        "    SUM(CASE WHEN updated >= '{2}' THEN revenue ELSE 0 END) as YTD_Revenue\n",
        "FROM local.db.customers\n",
        "\"\"\".format(start_of_month, start_of_quarter, start_of_year))\n",
        "\n",
        "revenue_stats.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "versioning-and-time-travel"
      },
      "source": [
        "## Versioning and Time Travel\n",
        "\n",
        "Demonstrate how to leverage Iceberg's time travel feature to query historical data states and manage schema evolution effectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "time-travel-example",
        "outputId": "b79f1024-4779-48a4-e6d2-87c4613ee06b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+-----+---------------------+-------+-------------------+----------+\n",
            "| id | name| email               |revenue| updated           |is_current|\n",
            "+----+-----+---------------------+-------+-------------------+----------+\n",
            "|  1 |Alice|alice@example.com    |  100.0|2021-06-01 00:00:00|      true|\n",
            "|  2 |Bob  |bob@example.com      |  150.0|2021-06-02 00:00:00|      true|\n",
            "+----+-----+---------------------+-------+-------------------+----------+\n"
          ]
        }
      ],
      "source": [
        "# Retrieve historical data by specifying a past snapshot\n",
        "historical_data = spark.read.format('iceberg').option('as-of-timestamp', snapshot_timestamp).table('local.db.customers')\n",
        "historical_data.show()\n",
        "\n",
        "# Evolving schema by adding a new column for tracking customer login\n",
        "spark.sql(\"ALTER TABLE local.db.customers ADD COLUMNS (last_login timestamp)\")\n",
        "print('Schema evolution applied.')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}