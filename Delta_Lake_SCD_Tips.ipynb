{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mbaroudi/DELTA_LAKE_TIPS/blob/main/Delta_Lake_SCD_Tips.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# Enhanced SCD Type 7 with Delta Lake and Spark SQL\n",
        "\n",
        "This notebook demonstrates the implementation of SCD Type 7 using Delta Lake and Spark SQL, showcasing ACID properties and historical data handling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup"
      },
      "outputs": [],
      "source": [
        "!pip install pyspark==3.1.2 delta-spark iceberg-spark\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from delta import *\n",
        "\n",
        "builder = SparkSession.builder \\\n",
        "    .appName(\"Enhanced SCD Type 7 Example\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
        "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
        "\n",
        "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "data-creation"
      },
      "source": [
        "## Data Creation\n",
        "\n",
        "Create an initial dataset and write it to a Delta table. This table will be used as the base for our SCD transformations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create-delta-table"
      },
      "outputs": [],
      "source": [
        "data = [\n",
        "    (1, \"Alice\", \"alice@example.com\", 1000, \"2020-01-01\"),\n",
        "    (2, \"Bob\", \"bob@example.com\", 1500, \"2020-01-01\")\n",
        "]\n",
        "columns = [\"customer_id\", \"name\", \"email\", \"revenue\", \"effective_date\"]\n",
        "\n",
        "df = spark.createDataFrame(data, schema=columns)\n",
        "df.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/delta_scd7\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "merge-changes"
      },
      "source": [
        "## Merge Changes Using Delta Table API\n",
        "\n",
        "Simulate incoming data changes and merge these using the Delta Table API to maintain historical records as per SCD Type 7."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "delta-merge"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import *\n",
        "\n",
        "new_data = [\n",
        "    (1, \"Alice\", \"alice@example.com\", 1100, \"2020-02-01\"),\n",
        "    (2, \"Bob\", \"bobby@example.com\", 1500, \"2020-02-01\"),\n",
        "    (3, \"Charlie\", \"charlie@example.com\", 500, \"2020-02-01\")\n",
        "]\n",
        "new_df = spark.createDataFrame(new_data, schema=columns)\n",
        "\n",
        "deltaTable = DeltaTable.forPath(spark, \"/tmp/delta_scd7\")\n",
        "\n",
        "deltaTable.alias(\"old\").merge(\n",
        "    new_df.alias(\"new\"),\n",
        "    \"old.customer_id = new.customer_id\"\n",
        ").whenMatchedUpdate(\n",
        "    condition=\"\"\"\n",
        "        old.email != new.email OR\n",
        "        old.revenue != new.revenue\n",
        "    \"\"\",\n",
        "    set={\n",
        "        \"name\": col(\"new.name\"),\n",
        "        \"email\": col(\"new.email\"),\n",
        "        \"revenue\": col(\"new.revenue\"),\n",
        "        \"effective_date\": current_date()\n",
        "    }\n",
        ").whenNotMatchedInsertAll().execute()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "query-data"
      },
      "source": [
        "## Querying Data with Spark SQL\n",
        "\n",
        "Enable querying the data using Spark SQL, demonstrating both the current state and historical views."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "spark-sql-queries"
      },
      "outputs": [],
      "source": [
        "# Register the Delta table as a SQL view\n",
        "spark.read.format(\"delta\").load(\"/tmp/delta_scd7\").createOrReplaceTempView(\"customer_data\")\n",
        "\n",
        "# Current state of the data\n",
        "spark.sql(\"\"\"\n",
        "SELECT * FROM customer_data\n",
        "WHERE effective_date = (SELECT MAX(effective_date) FROM customer_data)\n",
        "\"\"\").show()\n",
        "\n",
        "# Historical view, showing all changes\n",
        "spark.sql(\"\"\"\n",
        "SELECT * FROM customer_data\n",
        "ORDER BY customer_id, effective_date\n",
        "\"\"\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "time-travel"
      },
      "source": [
        "## Time Travel Query\n",
        "\n",
        "Delta Lakeâ€™s time travel feature allows querying previous snapshots of the dataset, useful for auditing and rollbacks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "time-travel-example"
      },
      "outputs": [],
      "source": [
        "# Query an older snapshot of the data\n",
        "version_number = 0  # adjust based on your versioning\n",
        "spark.read.format(\"delta\").option(\"versionAsOf\", version_number).load(\"/tmp/delta_scd7\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "best-practices"
      },
      "source": [
        "## Best Practices and Additional Tips\n",
        "\n",
        "- **Optimization**: Use `OPTIMIZE` and `ZORDER BY` commands to compact files and optimize data layout.\n",
        "- **Data Retention**: Configure data retention settings to manage old snapshots.\n",
        "- **Monitoring and Maintenance**: Regularly monitor and optimize performance.\n",
        "- **Incremental Loading**: Use patterns to efficiently load data batches."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}