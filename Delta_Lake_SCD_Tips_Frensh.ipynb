{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOmviuMK+nz4r613R4cmcYm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mbaroudi/DELTA_LAKE_TIPS/blob/main/Delta_Lake_SCD_Tips_Frensh.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dans le contexte de **PySpark** et de l'utilisation de **Delta Lake** pour gérer les dimensions avec des stratégies de dimension à évolution lente (SCD), le concept de **dimension « Type 7 »** n'est pas standard. Normalement, nous avons les types 1 à 6 qui couvrent divers scénarios de gestion des modifications des données dimensionnelles.\n",
        "Cependant, le « Type 7 » fait souvent référence à une **approche hybride** qui combine les fonctionnalités du Type 1 (écrasement des anciennes données) et du Type 2 (suivi des données historiques avec gestion des versions). Cette approche permet d'interroger à la fois l'état actuel des données et leurs versions historiques.\n",
        "\n",
        "Pour implémenter une dimension **SCD de type 7 dans PySpark à l'aide de Delta Lake**, vous implémenterez essentiellement un **SCD de type 2 **mais conserverez également une vue actuelle pour un accès plus facile aux derniers enregistrements. Vous trouverez ci-dessous un exemple de la façon dont vous pouvez configurer cela à l'aide de ***PySpark et Delta Lake*** :\n",
        "\n",
        "# Conditions préalables\n",
        "**Configuration d'Apache Spark et Delta Lake :** assurez-vous qu'Apache Spark et Delta Lake sont correctement configurés dans votre environnement.\n",
        "La bibliothèque de Delta Lake doit être incluse dans votre session Spark.\n",
        "Configuration initiale du DataFrame :\n",
        " * supposons que vous disposez d'un DataFrame avec des données client qui\n",
        "incluent ***un identifiant client, un nom_client, une adresse e-mail, revenu, une date_effet et un indicateur is_current***."
      ],
      "metadata": {
        "id": "Rn81tCsl_jtR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5TUcEV3q_e9b",
        "outputId": "f39ca249-fa5b-4999-9583-ac840f0b5d5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=78645a1f225b8e9e780e160038e97f693a622b52ee9f9e53cb3e97f9131a71a4\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"SCD Type 7 avec Aggrégats Temporels\") \\\n",
        "    #.config(\"spark.sql.warehouse.dir\", \"/path/to/hive/warehouse\") \\\n",
        "    #.enableHiveSupport() \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Préparez les données:** Insérez le code suivant pour initialiser vos DataFrames."
      ],
      "metadata": {
        "id": "ue55LbypBDgS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession, functions as F\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"SCD Type 7 with Temporal Aggregates\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Sample data with 'is_current' column added\n",
        "data = [\n",
        "    (1, \"John Doe\", \"john.doe@email.com\", 1000, \"2020-01-15\", True),\n",
        "    (2, \"Jane Smith\", \"jane.smith@email.com\", 1500, \"2020-01-20\", True)\n",
        "]\n",
        "columns = [\"customer_id\", \"customer_name\", \"email\", \"revenue\", \"effective_date\", \"is_current\"]\n",
        "df = spark.createDataFrame(data, schema=columns)\n",
        "\n",
        "new_data = [\n",
        "    (1, \"Johnathan Doe\", \"john.doe@email.com\", 1200, \"2020-02-01\", True),\n",
        "    (2, \"Jane Smith\", \"jane.smith@email.com\", 1500, \"2020-02-01\", True),\n",
        "    (3, \"Mike Jones\", \"mike.jones@email.com\", 500, \"2020-02-01\", True)\n",
        "]\n",
        "new_df = spark.createDataFrame(new_data, schema=columns)\n",
        "\n",
        "# Convert 'effective_date' to date type\n",
        "df = df.withColumn(\"effective_date\", F.to_date(F.col(\"effective_date\")))\n",
        "new_df = new_df.withColumn(\"effective_date\", F.to_date(F.col(\"effective_date\")))\n",
        "df.show()\n",
        "new_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yh61dpQTBN0i",
        "outputId": "bf3cdb71-35d6-42aa-e5d2-c54b09c7b6ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+--------------------+-------+--------------+----------+\n",
            "|customer_id|customer_name|               email|revenue|effective_date|is_current|\n",
            "+-----------+-------------+--------------------+-------+--------------+----------+\n",
            "|          1|     John Doe|  john.doe@email.com|   1000|    2020-01-15|      true|\n",
            "|          2|   Jane Smith|jane.smith@email.com|   1500|    2020-01-20|      true|\n",
            "+-----------+-------------+--------------------+-------+--------------+----------+\n",
            "\n",
            "+-----------+-------------+--------------------+-------+--------------+----------+\n",
            "|customer_id|customer_name|               email|revenue|effective_date|is_current|\n",
            "+-----------+-------------+--------------------+-------+--------------+----------+\n",
            "|          1|Johnathan Doe|  john.doe@email.com|   1200|    2020-02-01|      true|\n",
            "|          2|   Jane Smith|jane.smith@email.com|   1500|    2020-02-01|      true|\n",
            "|          3|   Mike Jones|mike.jones@email.com|    500|    2020-02-01|      true|\n",
            "+-----------+-------------+--------------------+-------+--------------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Appliquez SCD Type 7:** Utilisez le code suivant pour simuler les opérations SCD Type 7."
      ],
      "metadata": {
        "id": "_ZO13bXRBTzr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_scd_type_2(base_df, updates_df):\n",
        "    # Join based on customer_id and check for changes\n",
        "    condition = (base_df[\"customer_id\"] == updates_df[\"customer_id\"]) & \\\n",
        "                (base_df[\"is_current\"] == True) & \\\n",
        "                ((base_df[\"customer_name\"] != updates_df[\"customer_name\"]) |\n",
        "                 (base_df[\"revenue\"] != updates_df[\"revenue\"]))\n",
        "\n",
        "    # Set existing records to not current if changes are detected\n",
        "    updates_df = updates_df.withColumn(\"is_current\", F.lit(True))\n",
        "    updated_existing_df = base_df.join(updates_df, \"customer_id\", \"inner\") \\\n",
        "                                 .filter(condition) \\\n",
        "                                 .select(base_df[\"*\"]) \\\n",
        "                                 .withColumn(\"is_current\", F.lit(False))\n",
        "\n",
        "    # Union all: unchanged existing, updated existing set to false, and new updates set to true\n",
        "    final_df = base_df.join(updated_existing_df, [\"customer_id\"], \"left_anti\") \\\n",
        "                      .unionByName(updated_existing_df) \\\n",
        "                      .unionByName(updates_df)\n",
        "\n",
        "    return final_df\n",
        "\n",
        "historical_df = apply_scd_type_2(df, new_df)\n",
        "historical_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-z0yoNEnBcO3",
        "outputId": "c4d2b12f-6b59-4c47-eac7-026d42e9495a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+--------------------+-------+--------------+----------+\n",
            "|customer_id|customer_name|               email|revenue|effective_date|is_current|\n",
            "+-----------+-------------+--------------------+-------+--------------+----------+\n",
            "|          2|   Jane Smith|jane.smith@email.com|   1500|    2020-01-20|      true|\n",
            "|          1|     John Doe|  john.doe@email.com|   1000|    2020-01-15|     false|\n",
            "|          1|Johnathan Doe|  john.doe@email.com|   1200|    2020-02-01|      true|\n",
            "|          2|   Jane Smith|jane.smith@email.com|   1500|    2020-02-01|      true|\n",
            "|          3|   Mike Jones|mike.jones@email.com|    500|    2020-02-01|      true|\n",
            "+-----------+-------------+--------------------+-------+--------------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Analysez les Aggrégats Temporels:**"
      ],
      "metadata": {
        "id": "3bKJ1yhPDQWA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcul des agrégats temporels\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "historical_df = historical_df.withColumn(\"month\", F.month(\"effective_date\"))\n",
        "historical_df = historical_df.withColumn(\"quarter\", F.quarter(\"effective_date\"))\n",
        "historical_df = historical_df.withColumn(\"year\", F.year(\"effective_date\"))\n",
        "\n",
        "historical_df.createOrReplaceTempView(\"historical_data\")\n",
        "\n",
        "# Calcul de MTD, QTD, YTD\n",
        "aggregates_query = \"\"\"\n",
        "SELECT customer_id, customer_name,\n",
        "       SUM(CASE WHEN month = 2 AND year = 2020 THEN revenue ELSE 0 END) as MTD_Revenue,\n",
        "       SUM(CASE WHEN quarter = (SELECT quarter FROM historical_data WHERE month = 2 AND year = 2020 LIMIT 1) AND year = 2020 THEN revenue ELSE 0 END) as QTD_Revenue,\n",
        "       SUM(CASE WHEN year = 2020 THEN revenue ELSE 0 END) as YTD_Revenue\n",
        "FROM historical_data\n",
        "GROUP BY customer_id, customer_name\n",
        "\"\"\"\n",
        "aggregates_df = spark.sql(aggregates_query)\n",
        "aggregates_df.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4Wo7JtIDPI1",
        "outputId": "ef41295d-eb8b-48a7-f6b1-8750a4ecde75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+-----------+-----------+-----------+\n",
            "|customer_id|customer_name|MTD_Revenue|QTD_Revenue|YTD_Revenue|\n",
            "+-----------+-------------+-----------+-----------+-----------+\n",
            "|          2|   Jane Smith|       1500|       3000|       3000|\n",
            "|          1|     John Doe|          0|       1000|       1000|\n",
            "|          1|Johnathan Doe|       1200|       1200|       1200|\n",
            "|          3|   Mike Jones|        500|        500|        500|\n",
            "+-----------+-------------+-----------+-----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Écriture des données historiques (y compris toutes les versions) :**"
      ],
      "metadata": {
        "id": "llTP_4hzG1w6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stockage dans une table de faits Hive\n",
        "aggregates_df.write.mode(\"overwrite\").saveAsTable(\"fact_revenue_aggregates\")\n",
        "# Pour ajouter des données au lieu de les écraser, vous pouvez utiliser mode(\"append\")"
      ],
      "metadata": {
        "id": "Pj96lDeBGgJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Rédaction des enregistrements actuels :**"
      ],
      "metadata": {
        "id": "-0C33v5EG5Du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtrage uniquement des enregistrements actuels\n",
        "current_records_df = historical_df.filter(\"is_current = True\")\n",
        "\n",
        "# Écrivez ou écrasez les données de l'enregistrement actuel dans une autre table Hive\n",
        "current_records_df.write.mode(\"overwrite\").saveAsTable(\"hive_current_records\")\n"
      ],
      "metadata": {
        "id": "EM2f4VqPHC_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consommation de données pour les outils de tableau de bord\n",
        "Une fois vos données stockées dans Hive, elles sont accessibles par divers outils de BI et de tableaux de bord qui se connectent à Hive. La configuration implique généralement :\n",
        "\n",
        "**Connexion de l'outil à Hive :** configurez votre outil de tableau de bord (Microstrat) pour vous connecter à votre base de données Hive.\n",
        "Interrogation de données : utilisez des requêtes SQL dans l'outil pour récupérer et visualiser les données, en tirant parti des tables **hive_historical_data** et **hive_current_records**.\n",
        "\n",
        "## Inconvénients de la Méthode Full Overwrite sur Hive\n",
        "\n",
        "### Risque de Perte de Données :\n",
        "\n",
        "Lorsque vous écrasez une table, il y a un court intervalle durant lequel les données ne sont pas disponibles, ce qui peut être problématique pour les opérations en continu. De plus, en cas d'erreur pendant l'opération d'écriture, il est possible que les données soient partiellement écrites ou corrompues, ce qui pourrait entraîner une perte de données.\n",
        "### Utilisation Inefficace des Ressources :\n",
        "Écraser une table entière à chaque mise à jour est une opération très coûteuse en termes de traitement et de stockage, surtout si seules quelques lignes ou colonnes nécessitent une actualisation. Cela entraîne une utilisation inefficace des ressources de calcul et de stockage.\n",
        "### Performance :\n",
        "La performance peut être sérieusement affectée, surtout avec de grandes tables. Le processus de rechargement complet peut être très long, ce qui retarde la disponibilité des données mises à jour pour les utilisateurs et les processus en aval.\n",
        "### Impact sur les Utilisateurs en Aval :\n",
        "Pendant que la table est en cours de surcharge, les utilisateurs qui dépendent de cette table pour des rapports ou des analyses peuvent se retrouver sans données ou avec des données incomplètes, impactant ainsi leur capacité à effectuer des tâches critiques.\n",
        "### Complexité de la Gestion des Versions :\n",
        "Si des versions historiques des données sont nécessaires pour l'audit ou d'autres analyses rétrospectives, les gérer devient plus compliqué avec une approche de surcharge. Chaque surcharge efface l'historique antérieur, à moins que des mesures supplémentaires ne soient prises pour archiver ces données.\n",
        "### Considérations de Synchronisation :\n",
        "Si la table est utilisée par plusieurs utilisateurs ou processus, synchroniser les accès pour éviter les lectures de données en état de transition peut devenir une tâche administrative complexe."
      ],
      "metadata": {
        "id": "sQew-8YGHiak"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Un nouveau concept Delta Lake avec PySpark"
      ],
      "metadata": {
        "id": "x1QlSRF-NLNI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark==3.1.2 delta-spark\n"
      ],
      "metadata": {
        "id": "_l31wnnWNXHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0t0nFo56Nb0v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Étape 2 : Configuration de la Session Spark**\n",
        "Après avoir installé les packages nécessaires, configurez votre session Spark pour utiliser Delta Lake :"
      ],
      "metadata": {
        "id": "QU0qYhpqNgM3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark==3.1.2 delta-spark\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "builder = SparkSession.builder.appName(\"SCD Type 7 with Temporal Aggregates\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
        "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
        "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:1.0.0\")\n",
        "\n",
        "spark = builder.getOrCreate()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avHlJmzGNfHm",
        "outputId": "6fe87e9e-26d4-47fd-fd7d-1d1bb3825f83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark==3.1.2 in /usr/local/lib/python3.10/dist-packages (3.1.2)\n",
            "Requirement already satisfied: delta-spark in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
            "Requirement already satisfied: py4j==0.10.9 in /usr/local/lib/python3.10/dist-packages (from pyspark==3.1.2) (0.10.9)\n",
            "Requirement already satisfied: importlib-metadata>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from delta-spark) (7.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=3.10.0->delta-spark) (3.18.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Étape 3 : Utilisation de Delta Lake**\n",
        "Avec la session configurée, vous pouvez maintenant créer des DataFrames, les écrire en format Delta, et effectuer des mises à jour ou des suppressions si nécessaire. Voici comment procéder :"
      ],
      "metadata": {
        "id": "1FM3nhF5N2Gb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession, functions as F\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, BooleanType\n",
        "\n",
        "# Initialize Spark Session with Delta Lake\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"SCD Type 7 with Temporal Aggregates\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
        "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Define schema without DateType for initial creation\n",
        "schema = StructType([\n",
        "    StructField(\"customer_id\", IntegerType(), True),\n",
        "    StructField(\"customer_name\", StringType(), True),\n",
        "    StructField(\"email\", StringType(), True),\n",
        "    StructField(\"revenue\", IntegerType(), True),\n",
        "    StructField(\"effective_date\", StringType(), True),  # Temporarily as StringType\n",
        "    StructField(\"is_current\", BooleanType(), True)\n",
        "])\n",
        "\n",
        "# Sample data, keeping dates as strings initially\n",
        "data = [\n",
        "    (1, \"John Doe\", \"john.doe@email.com\", 1000, \"2020-01-15\", True),\n",
        "    (2, \"Jane Smith\", \"jane.smith@email.com\", 1500, \"2020-01-20\", True)\n",
        "]\n",
        "\n",
        "# Create DataFrame with dates as strings\n",
        "df = spark.createDataFrame(data, schema=schema)\n",
        "\n",
        "# Convert 'effective_date' from string to DateType within the DataFrame\n",
        "df = df.withColumn(\"effective_date\", F.to_date(F.col(\"effective_date\")))\n",
        "\n",
        "df.show()\n",
        "\n",
        "# Define a path for the Delta table\n",
        "delta_path = \"/tmp/delta-table\"\n",
        "\n",
        "# Write the DataFrame to Delta format\n",
        "df.write.format(\"delta\").mode(\"overwrite\").save(delta_path)\n",
        "\n",
        "# Read the DataFrame back as a DeltaTable\n",
        "from delta.tables import DeltaTable\n",
        "delta_table = DeltaTable.forPath(spark, delta_path)\n",
        "\n",
        "# New data, converted dates within the DataFrame creation\n",
        "new_data = [\n",
        "    (1, \"Johnathan Doe\", \"john.doe@email.com\", 1200, \"2020-02-01\", True),\n",
        "    (2, \"Jane Smith\", \"jane.smith@email.com\", 1500, \"2020-02-01\", True),\n",
        "    (3, \"Mike Jones\", \"mike.jones@email.com\", 500, \"2020-02-01\", True)\n",
        "]\n",
        "\n",
        "new_df = spark.createDataFrame(new_data, schema=schema)\n",
        "new_df = new_df.withColumn(\"effective_date\", F.to_date(F.col(\"effective_date\")))\n",
        "\n",
        "new_df.show()\n",
        "# Perform the merge operation\n",
        "delta_table.alias(\"old\").merge(\n",
        "    new_df.alias(\"new\"),\n",
        "    \"old.customer_id = new.customer_id\"\n",
        ").whenMatchedUpdate(\n",
        "    condition = \"\"\"\n",
        "        old.is_current = TRUE AND (\n",
        "        old.customer_name != new.customer_name OR\n",
        "        old.email != new.email OR\n",
        "        old.revenue != new.revenue)\n",
        "    \"\"\",\n",
        "    set = {\n",
        "        \"customer_name\": F.col(\"new.customer_name\"),\n",
        "        \"email\": F.col(\"new.email\"),\n",
        "        \"revenue\": F.col(\"new.revenue\"),\n",
        "        \"effective_date\": F.col(\"new.effective_date\"),\n",
        "        \"is_current\": F.lit(False)\n",
        "    }\n",
        ").whenNotMatchedInsertAll().execute()\n",
        "\n",
        "# Show the updated results\n",
        "updated_df = spark.read.format(\"delta\").load(delta_path)\n",
        "updated_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKMhDYLINxJn",
        "outputId": "05cd3d68-85a2-4ab3-d5f2-045ab0cf4987"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+--------------------+-------+--------------+----------+\n",
            "|customer_id|customer_name|               email|revenue|effective_date|is_current|\n",
            "+-----------+-------------+--------------------+-------+--------------+----------+\n",
            "|          1|     John Doe|  john.doe@email.com|   1000|    2020-01-15|      true|\n",
            "|          2|   Jane Smith|jane.smith@email.com|   1500|    2020-01-20|      true|\n",
            "+-----------+-------------+--------------------+-------+--------------+----------+\n",
            "\n",
            "+-----------+-------------+--------------------+-------+--------------+----------+\n",
            "|customer_id|customer_name|               email|revenue|effective_date|is_current|\n",
            "+-----------+-------------+--------------------+-------+--------------+----------+\n",
            "|          1|Johnathan Doe|  john.doe@email.com|   1200|    2020-02-01|      true|\n",
            "|          2|   Jane Smith|jane.smith@email.com|   1500|    2020-02-01|      true|\n",
            "|          3|   Mike Jones|mike.jones@email.com|    500|    2020-02-01|      true|\n",
            "+-----------+-------------+--------------------+-------+--------------+----------+\n",
            "\n",
            "+-----------+-------------+--------------------+-------+--------------+----------+\n",
            "|customer_id|customer_name|               email|revenue|effective_date|is_current|\n",
            "+-----------+-------------+--------------------+-------+--------------+----------+\n",
            "|          1|Johnathan Doe|  john.doe@email.com|   1200|    2020-02-01|     false|\n",
            "|          3|   Mike Jones|mike.jones@email.com|    500|    2020-02-01|      true|\n",
            "|          2|   Jane Smith|jane.smith@email.com|   1500|    2020-01-20|      true|\n",
            "+-----------+-------------+--------------------+-------+--------------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Il est tout à fait possible de combiner deux approches *(Utilisation de Delta Lake pour Emuler des Opérations Transactionnelles / Utilisation de Tableaux Externes et Vues pour Simuler des Updates)* pour gérer les limitations de Hive en matière de mises à jour et de suppressions de données.\n",
        "\n",
        "En mélangeant les choix 1 (utilisation de Delta Lake pour émuler des opérations transactionnelles) et 4 (utilisation de tableaux externes et de vues pour simuler des updates), vous pouvez créer un système hybride qui optimise les capacités de votre infrastructure existante tout en apportant une flexibilité accrue pour la gestion des données.\n",
        "\n",
        "**Voici comment vous pourriez procéder :**\n",
        "\n",
        "# Combinaison des Approches avec Delta Lake et Hive\n",
        "\n",
        "## Utilisation de Delta Lake comme Couche de Traitement Primaire :\n",
        " **Stockage Temporaire :**\n",
        "\n",
        " Utilisez Delta Lake pour gérer les données de manière transactionnelle dans une zone de stockage temporaire. Cela vous permet d'utiliser les fonctionnalités de mise à jour, de suppression et de merge qui ne sont pas disponibles dans Hive.\n",
        "\n",
        "**Transformation et Préparation :**\n",
        "\n",
        "Effectuez toutes les transformations nécessaires, y compris les mises à jour et les nettoyages de données, dans cette couche avant de les transférer à Hive.\n",
        "\n",
        "# **Exportation vers Hive en Utilisant des Tables Externes :**\n",
        "\n",
        "**Création de Tables Externes :**\n",
        "\n",
        "Une fois les données prêtes dans Delta Lake, exportez-les sous forme de fichiers Parquet (ou un autre format optimal pour votre cas d'usage) dans un répertoire accessible par Hive.\n",
        "Tables Hive sur Fichiers Externes : Créez des tables Hive qui pointent vers ces fichiers externes. Ces tables ne seront pas transactionnelles, mais elles bénéficieront de la préparation et des mises à jour effectuées dans la couche Delta Lake.\n",
        "\n",
        "**Utilisation de Vues pour Simuler les Mises à Jour :**\n",
        "\n",
        "**Création de Vues :**\n",
        "\n",
        "Dans Hive, créez des vues qui simulent les versions les plus récentes des données en utilisant des clauses SQL pour filtrer les enregistrements selon les critères que vous auriez définis (par exemple, le timestamp le plus récent, les flags de version, etc.).\n",
        "Vues Aggregées : Si nécessaire, utilisez des vues pour créer des agrégats ou des synthèses qui sont régulièrement mises à jour par vos opérations dans Delta Lake et répercutées dans Hive via les nouvelles écritures de fichiers.\n",
        "Avantages de cette Approche Hybride\n",
        "\n",
        "**Flexibilité :**\n",
        "\n",
        "Vous combinez la flexibilité transactionnelle de Delta Lake avec la capacité de stockage et l'écosystème analytique étendu de Hive.\n",
        "\n",
        "**Performance :**\n",
        "\n",
        "Les opérations lourdes et les mises à jour sont gérées en amont dans un environnement optimisé (Delta Lake), améliorant ainsi la performance des requêtes exécutées sur Hive.\n",
        "\n",
        "**Sécurité des Données :**\n",
        "\n",
        "Les données peuvent être préparées et nettoyées efficacement avant d'être exposées via Hive, réduisant le risque d'erreurs ou de données obsolètes."
      ],
      "metadata": {
        "id": "PjtJIHzIWAFv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuration de l'Environnement Spark avec Iceberg"
      ],
      "metadata": {
        "id": "dwcORivSSchC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!wget https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-spark3-runtime/0.11.0/iceberg-spark3-runtime-0.11.0.jar\n",
        "#!wget https://repo1.maven.org/maven2/org/apache/spark/spark-sql_2.12/3.0.1/spark-sql_2.12-3.0.1.jar\n",
        "\n",
        "import os\n",
        "os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars /content/iceberg-spark3-runtime-0.11.0.jar,/content/spark-sql_2.12-3.0.1.jar pyspark-shell'\n"
      ],
      "metadata": {
        "id": "LiMHZK3xTs6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install pyspark\n",
        "#!wget https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-spark-runtime-3.2_2.12/0.13.1/iceberg-spark-runtime-3.2_2.12-0.13.1.jar\n"
      ],
      "metadata": {
        "id": "NJElzST6UhFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars /content/iceberg-spark-runtime-3.2_2.12-0.13.1.jar pyspark-shell'\n"
      ],
      "metadata": {
        "id": "avv8hxpxUi2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!wget -q https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-spark-runtime-3.1_2.12/0.13.1/iceberg-spark-runtime-3.1_2.12-0.13.1.jar -O iceberg-runtime.jar\n"
      ],
      "metadata": {
        "id": "TLOEzBYoWAoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars /content/iceberg-runtime.jar pyspark-shell'\n"
      ],
      "metadata": {
        "id": "Jr5GA0QIWCZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = (\n",
        "    \"--packages org.apache.iceberg:iceberg-spark3-runtime:0.13.1 \"\n",
        "    \"--repositories https://repo.maven.apache.org/maven2 \"\n",
        "    \"pyspark-shell\"\n",
        ")\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Iceberg Integration\") \\\n",
        "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
        "    .config(\"spark.sql.catalog.local\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
        "    .config(\"spark.sql.catalog.local.type\", \"hadoop\") \\\n",
        "    .config(\"spark.sql.catalog.local.warehouse\", \"file:///content/iceberg_warehouse\") \\\n",
        "    .getOrCreate()\n"
      ],
      "metadata": {
        "id": "IQ7ZzBTtVf0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dropper la table Iceberg if is not exist\n",
        "spark.sql(\"\"\"\n",
        "DROP table local.db.table_name\n",
        "\"\"\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dk0z3xjUr2-2",
        "outputId": "3b0a469e-363c-4ef9-b8ef-9125188c8fca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Création d'une table Iceberg if is not exist\n",
        "spark.sql(\"\"\"\n",
        " CREATE TABLE local.db.table_name (\n",
        "    id int,\n",
        "    data string\n",
        ") USING iceberg\n",
        "\"\"\")\n",
        "\n",
        "# Insertion de données\n",
        "spark.sql(\"\"\"\n",
        "INSERT INTO local.db.table_name VALUES (1, 'hello'), (2, 'world')\n",
        "\"\"\")\n",
        "\n",
        "# Lecture des données\n",
        "df = spark.sql(\"SELECT * FROM local.db.table_name\")\n",
        "df.show()\n"
      ],
      "metadata": {
        "id": "hyIrTS8IX_wI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install pyspark\n",
        "#!wget -q https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-spark-runtime-3.1_2.12/0.13.1/iceberg-spark-runtime-3.1_2.12-0.13.1.jar -O iceberg-runtime.jar\n",
        "\n",
        "import os\n",
        "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"--packages org.apache.iceberg:iceberg-spark3-runtime:0.13.1 --repositories https://repo.maven.apache.org/maven2 pyspark-shell\"\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Iceberg Integration\") \\\n",
        "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
        "    .config(\"spark.sql.catalog.local\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
        "    .config(\"spark.sql.catalog.local.type\", \"hadoop\") \\\n",
        "    .config(\"spark.sql.catalog.local.warehouse\", \"file:///content/iceberg_warehouse\") \\\n",
        "    .getOrCreate()\n"
      ],
      "metadata": {
        "id": "Z325EZ2jTNiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "schema = \"\"\"\n",
        "DROP TABLE IF EXISTS local.default.customer_data\n",
        "\"\"\"\n",
        "spark.sql(schema)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7au1TAdydxQe",
        "outputId": "2f6374f2-7cbd-4bf1-ef29-03a5cd5b34fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "schema = \"\"\"\n",
        "CREATE TABLE IF NOT EXISTS local.default.customer_data (\n",
        "    customer_id INT,\n",
        "    customer_name STRING,\n",
        "    email STRING,\n",
        "    revenue INT,\n",
        "    effective_date DATE,\n",
        "    is_current BOOLEAN,\n",
        "    version INT,\n",
        "    end_date DATE\n",
        ") USING iceberg\n",
        "\"\"\"\n",
        "\n",
        "spark.sql(schema)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlpAirayY5G1",
        "outputId": "cdcaf682-514f-4999-9e45-9528e46a0432"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import to_date, col\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, BooleanType, DateType\n",
        "\n",
        "# Establishing Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"SCD2 Example using Iceberg\") \\\n",
        "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
        "    .config(\"spark.sql.catalog.local\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
        "    .config(\"spark.sql.catalog.local.type\", \"hadoop\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Define the schema with StringType for date fields\n",
        "schema = StructType([\n",
        "    StructField(\"customer_id\", IntegerType(), True),\n",
        "    StructField(\"customer_name\", StringType(), True),\n",
        "    StructField(\"email\", StringType(), True),\n",
        "    StructField(\"revenue\", IntegerType(), True),\n",
        "    StructField(\"effective_date\", StringType(), True),  # Use StringType initially\n",
        "    StructField(\"is_current\", BooleanType(), True),\n",
        "    StructField(\"version\", IntegerType(), True),\n",
        "    StructField(\"end_date\", StringType(), True)  # Use StringType initially\n",
        "])\n",
        "\n",
        "# Creating new data with date initially as string\n",
        "new_data = [\n",
        "    (1, \"John Doe Updated\", \"john.doe@update.com\", 1500, \"2021-05-01\", True, 2, None),  # Date as string\n",
        "    (1, \"John Doe\", \"john.doe@email.com\", 1900, \"2020-01-01\", True, 1, None),\n",
        "    (1, \"John Doe\", \"john.new@email.com\", 1000, \"2020-06-01\", False, 2, \"2021-01-01\"),\n",
        "    (1, \"John Doe II\", \"john.newII@email.com\", 1200, \"2021-01-02\", True, 3, None),\n",
        "    (2, \"Jane Smith\", \"jane.smith@email.com\", 1500, \"2020-01-01\", False, 1, \"2020-09-30\"),\n",
        "    (2, \"Jane S. Doe\", \"jane.s.doe@email.com\", 1600, \"2020-10-01\", True, 2, None),\n",
        "    (3, \"Bob Brown\", \"bob.brown@email.com\", 800, \"2020-01-01\", False, 1, \"2020-08-15\"),\n",
        "    (3, \"Bobby Brown\", \"bobby.br@email.com\", 950, \"2020-08-16\", True, 2, None)\n",
        "]\n",
        "\n",
        "# Create DataFrame with string dates\n",
        "df = spark.createDataFrame(new_data, schema=schema)\n",
        "\n",
        "# Convert 'effective_date' and 'end_date' from string to DateType using to_date function\n",
        "df = df.withColumn(\"effective_date\", to_date(col(\"effective_date\"), \"yyyy-MM-dd\"))\n",
        "df = df.withColumn(\"end_date\", to_date(col(\"end_date\"), \"yyyy-MM-dd\"))\n",
        "\n",
        "# Showing the DataFrame to verify the results\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3YL4cUP8_niJ",
        "outputId": "e4011edb-b0c6-4872-bfbf-74a32bd96faa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------------+--------------------+-------+--------------+----------+-------+----------+\n",
            "|customer_id|   customer_name|               email|revenue|effective_date|is_current|version|  end_date|\n",
            "+-----------+----------------+--------------------+-------+--------------+----------+-------+----------+\n",
            "|          1|John Doe Updated| john.doe@update.com|   1500|    2021-05-01|      true|      2|      null|\n",
            "|          1|        John Doe|  john.doe@email.com|   1900|    2020-01-01|      true|      1|      null|\n",
            "|          1|        John Doe|  john.new@email.com|   1000|    2020-06-01|     false|      2|2021-01-01|\n",
            "|          1|     John Doe II|john.newII@email.com|   1200|    2021-01-02|      true|      3|      null|\n",
            "|          2|      Jane Smith|jane.smith@email.com|   1500|    2020-01-01|     false|      1|2020-09-30|\n",
            "|          2|     Jane S. Doe|jane.s.doe@email.com|   1600|    2020-10-01|      true|      2|      null|\n",
            "|          3|       Bob Brown| bob.brown@email.com|    800|    2020-01-01|     false|      1|2020-08-15|\n",
            "|          3|     Bobby Brown|  bobby.br@email.com|    950|    2020-08-16|      true|      2|      null|\n",
            "+-----------+----------------+--------------------+-------+--------------+----------+-------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import to_date, col, lit, current_date\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, BooleanType, DateType\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "\n",
        "# Define a window specification to get the latest record for deduplication\n",
        "window_spec = Window.partitionBy(\"customer_id\").orderBy(col(\"version\").desc())\n",
        "\n",
        "# Creating a data set that includes updates (marking old records as not current)\n",
        "updates = current_data.withColumn(\"is_current\", lit(False)) \\\n",
        "                      .withColumn(\"end_date\", lit(current_date()))\n",
        "\n",
        "# Append new data to updates without duplicates\n",
        "deduplicated_data = new_df.withColumn(\"row_number\", row_number().over(window_spec)) \\\n",
        "                          .filter(col(\"row_number\") == 1).drop(\"row_number\")\n",
        "\n",
        "# Handle condition: if record exists in both new and old data, take from new data and update old\n",
        "final_data = updates.unionByName(deduplicated_data).dropDuplicates([\"customer_id\"])\n",
        "\n",
        "# Write results back to the Iceberg table\n",
        "final_data.write.format(\"iceberg\").mode(\"append\").option(\"overwrite-mode\", \"dynamic\").saveAsTable(\"local.default.customer_data\")\n",
        "\n",
        "# Verify the results\n",
        "result_df = spark.table(\"local.default.customer_data\")\n",
        "result_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZxWC36pePNY",
        "outputId": "3557a79f-3c48-4c2b-9611-dcd58aa0c22a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------------+--------------------+-------+--------------+----------+-------+--------+\n",
            "|customer_id|   customer_name|               email|revenue|effective_date|is_current|version|end_date|\n",
            "+-----------+----------------+--------------------+-------+--------------+----------+-------+--------+\n",
            "|          1|John Doe Updated| john.doe@update.com|   1100|    2021-05-01|      true|      2|    null|\n",
            "|          3|    New Customer|new.customer@emai...|    500|    2021-05-01|      true|      1|    null|\n",
            "+-----------+----------------+--------------------+-------+--------------+----------+-------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result_df = spark.sql(\"SELECT * FROM local.default.customer_data\")\n",
        "result_df.show()\n"
      ],
      "metadata": {
        "id": "cB3YlcUAguO_",
        "outputId": "9f3abc50-bf4b-4175-bbc9-31e3af4cd4e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------------+--------------------+-------+--------------+----------+-------+--------+\n",
            "|customer_id|   customer_name|               email|revenue|effective_date|is_current|version|end_date|\n",
            "+-----------+----------------+--------------------+-------+--------------+----------+-------+--------+\n",
            "|          1|John Doe Updated| john.doe@update.com|   1100|    2021-05-01|      true|      2|    null|\n",
            "|          3|    New Customer|new.customer@emai...|    500|    2021-05-01|      true|      1|    null|\n",
            "+-----------+----------------+--------------------+-------+--------------+----------+-------+--------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}